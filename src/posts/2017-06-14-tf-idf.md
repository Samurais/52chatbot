---
layout: post
title: "TF-IDF"
excerpt: "词频和逆文档频率算法简单快速，结果处理符合实际情况，可以用在关键词提取，信息检索等很多地方。"
category: development
tags: [nlp]
disqus: true
---

如果我们有一篇很长的文章，如何获得关键词呢？
根据信息熵理论，一个词出现的次数越多，这个词包含的信息量就越小。可以说，TF-IDF算法就是基于这一理论的。

这篇文章我们称之为Document, 这篇文章属于一个 Collection(集合)。

## 处理

将Document进行[分词](http://nlp.chatbot.io/public/index.html)，去停留词。

1) 计算TF
词频(TF) = 某个词在文章中出现次数

> 标准化：词频(TF) = 某个词在文章中出现的次数 / 文章的总词数

2) 计算IDF

逆文档频率(IDF) = log(collection总文档数 / (包含该词的文档数 + 1))

> 使用了 add-1 方法，避免 0 分母

3) 计算TF-IDF值

TF-IDF = TF x IDF

TF-IDF与一个词在Document中出现的次数成正比，与该词在Collection中出现的次数成反比。

## 提取关键词
设定一个Collection, 比如使用Google进行检索。
对Document的每个词做TF-IDF，降序排列。排在前面的就是关键词。

> 如果我们只想分析一个词的重要性，而不针对文章，我们可以单独的使用IDF值。

## 缺点
单纯以"词频"为基础计算一个词的重要性。


## 参考
[TF-IDF与余弦相似性的应用（一）：自动提取关键词](http://www.ruanyifeng.com/blog/2013/03/tf-idf.html)

