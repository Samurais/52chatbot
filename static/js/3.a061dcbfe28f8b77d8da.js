webpackJsonp([3,53],{346:function(e,t){e.exports={rawContent:"\nMaximum Likelihood Estimation for Linear Regression with Tensorflow\n# 了解似然函数\nhttps://en.wikipedia.org/wiki/Maximum_likelihood_estimation\n\n# 最大似然算法\nhttps://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0\n\n# TensorFlow线性回归的例子\nhttps://stackoverflow.com/questions/41885665/maximum-likelihood-linear-regression-tensorflow\n\nexample:\n\n```python\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#===============================================================================\n#\n# Copyright (c) 2017 Hai Liang Wang, All Rights Reserved\n#\n#\n# File: linear_regression_tensorflow.py\n# Author: Hai Liang Wang\n# Date: 2017-08-01:15:03:17\n#\n#===============================================================================\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    # Ops and variables pinned to the CPU because of missing GPU implementation\n    with tf.device('/cpu:0'):\n        X = tf.placeholder(\"float\", None)\n        Y = tf.placeholder(\"float\", None)\n        theta_0 = tf.Variable(np.random.randn())\n        theta_1 = tf.Variable(np.random.randn())\n        var = tf.Variable(0.5)\n\n        # y = theta_0 + (x * theta_1)\n        hypothesis = tf.add(theta_0, tf.mul(X, theta_1))\n        lhf = 1 * (50 * np.log(2*np.pi) + 50 * tf.log(var) + (1/(2*var)) * tf.reduce_sum(tf.pow(hypothesis - Y, 2)))\n        op = tf.train.GradientDescentOptimizer(0.01).minimize(lhf)\n\n        # Add variable initializer.\n        init = tf.global_variables_initializer()\n\nwith tf.Session(graph=graph) as session:\n    # We must initialize all variables before we use them.\n    init.run()\n    print(\"Initialized\")\n\n    train_X = np.random.rand(100, 1) # all values [0-1)\n    train_Y = train_X\n    feed_dict = {X: train_X, Y: train_Y}\n    num_steps = 100001\n\n    for steps in range(num_steps):\n        _, loss, v0, v1 = session.run([op, lhf, theta_0, theta_1], feed_dict=feed_dict)\n        # since x == y, theta_0 should be 0 and theta_1 should be 1.\n        print('Run step %s, loss %s, theta_0 %s, theta_1 %s' % (steps, loss, v0, v1))\n```\n\n![](http://7xkeqi.com1.z0.glb.clouddn.com/chatbot/images/2017/08/maximum-likelihood-estimation-1.png)\n\n\n# minst dataset with log Likelihood_function\nMaximum likelihood estimation from https://github.com/bond005/python-logreg\n\nThe logreg.py module was created for educational purposes for practical lessons on \nthe course \"Methods and algorithms of computer linguistics\", read to students of \nthe Faculty of Humanities at Novosibirsk State University.\n\nIn the logreg.py module, a logistic regression algorithm is implemented to solve \nthe classification problem. The process of learning logistic regression is considered \nas maximization of the logarithm of the likelihood function by the gradient method.\n\nThe logreg.py module is developed in the Python 3.x/2.x programming language using the \nNumPy Python library. The module can be used as a component of any Python-program, \nin which it is necessary to solve the problem of classification using the algorithm \nof logistic regression. In addition, this module can also be launched as a standalone \nprogram: in this case, the learning process for recognizing handwritten digits from the \ntraining subsample of the MNIST case will be demonstrated, and then - the process of \nrecognizing the handwritten digits from the MNIST test sub-sample. \n\nTo run the logreg.py module as a stand-alone program, you must install the Python-library \nScikitLearn in the system (the tools of this library allow access to MNIST).\n\n```python\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport functools\nimport math # we import a standard python library for mathematics\nimport numpy # import the NumPy library to work with NumPy arrays of class numpy.ndarray\nimport scipy.sparse # import the Sparse package for SciPy for sparse matrices\n\nclass LogRegError(Exception):\n    '''\n    The exception class that we generate, if something goes wrong in the logistic regression.\n    '''\n    def __init__(self, error_msg=None):\n        \"\"\" \n        A class constructor that is called automatically when creating class objects.\n         : Param error_msg - The error message that we want to send, throwing an exception.\n        \"\"\"\n        self.msg = error_msg\n\n    def __str__(self):\n        \"\"\"A standard method that is called automatically when printing objects of a class using the print function.\n         : Return Returns the string to print: an error message common to logistic regression,\n         And an additional message passed as the constructor argument (see comment to the constructor).\n         \"\"\"\n        error_msg = 'Logistic regression algorithm is incorrect!'\n        if self.msg is not None:\n            error_msg += (' ' + self.msg)\n        return error_msg\n\n\nclass LogisticRegression:\n    '''\n    Class for the classifier based on the logistic regression algorithm.\n    '''\n\n    def __init__(self):\n        '''\n        A class constructor that is called automatically when creating class objects.\n        In the constructor, we initialize all the class attributes with \"empty\" values.\n        '''\n        self.__a = None # attribute of the class that will be a free member of the logistic regression\n        self.__b = None  # attribute of the class that will be a numpy.ndarray array of logistic regression coefficients\n        self.__th = None  # attribute of the class that will be the probabilistic threshold for classification\n\n    def save(self, file_name):\n        '''\n        Save all logistic regression parameters (class attributes) to a text file.\n        : Param file_name - a string with the name of the text file, in which the saved parameters will be written.\n        '''\n        # First check if there is anything to save\n        if (self.__a is None) or (self.__b is None) or (self.__th is None):\n            # If the class attributes are empty, i.e. There is nothing to save, then we throw an exception\n            raise LogRegError('Parameters have not been specified!')\n        # Open a text file for writing\n        with open(file_name, 'w') as fp:\n            # Write the size of the input characteristic vector\n            fp.write('Input size {0}\\n\\n'.format(self.__b.shape[0]))\n            # Write the coefficients of logistic regression\n            for ind in range(self.__b.shape[0]):\n                fp.write('{0}\\n'.format(self.__b[ind]))\n            # Write the free term and the probabilistic threshold\n            fp.write('\\n{0}\\n\\n{1}\\n'.format(self.__a, self.__th))\n\n    def load(self, file_name):\n        '''\n        Load all the logistic regression parameters from the text file into the attributes of the class.\n        : Param file_name - a string with the name of the text file from which the downloaded parameters will be read.\n        '''\n        # Open a text file for reading\n        with open(file_name, 'r') as fp:\n            input_size = -1 # the size of the input characteristic vector (until read, is set to -1)\n            cur_line = fp.readline()  # read the first line\n            ind = 0  # counter of the number of logistic regression parameters read\n            while len(cur_line) > 0:  # until the next line is empty, i.e. The file is not over yet\n                prepared_line = cur_line.strip()   # remove extra spaces from the beginning and end of the line\n                if len(prepared_line) > 0:  # if after removing spaces the line is not empty, then we try to parse it\n                    if input_size <= 0: # if the size of the input characteristic vector has not yet been read, then read it\n                        parts_of_line = prepared_line.split()\n                        if len(parts_of_line) != 3:\n                            raise LogRegError('Parameters cannot be loaded from a file!')\n                        if (parts_of_line[0].lower() != 'input') or (parts_of_line[1].lower() != 'size'):\n                            raise LogRegError('Parameters cannot be loaded from a file!')\n                        input_size = int(parts_of_line[2])\n                        if input_size <= 0:\n                            raise LogRegError('Parameters cannot be loaded from a file!')\n                        self.__b = numpy.zeros(shape=(input_size,), dtype=numpy.float)\n                        self.__a = 0.0\n                        self.__th = 0.5\n                    else:  # if the size of the input characteristic vector has already been read, then we read the regression parameters themselves\n                        if ind > (input_size + 1):  # the file contained too much information, this is an error\n                            raise LogRegError('Parameters cannot be loaded from a file!')\n                        if ind < input_size: # read ind-th logistic regression coefficient from input_size pieces\n                            self.__b[ind] = float(prepared_line)\n                        elif ind == input_size: # read the free member of the logistic regression\n                            self.__a = float(prepared_line)\n                        else:  # read the probability threshold (it should not be less than 0 or greater than 1)\n                            self.__th = float(prepared_line)\n                            if (self.__th < 0.0) or (self.__th > 1.0):\n                                raise LogRegError('Parameters cannot be loaded from a file!')\n                        ind += 1 # safely read the next parameter, and now we increase the counter\n                cur_line = fp.readline() # read the next line from the file\n            if ind <= (input_size + 1):\n                raise LogRegError('Parameters cannot be loaded from a file!')\n\n    def transform(self, X):\n        '''\n        Calculate the probabilities of assigning input objects to the first class.\n        : Param X - a two-dimensional numpy.ndarray-array that describes the vectors of attributes of input objects\n        (One line is one characteristic vector, the number of rows is equal to the number of input objects,\n        The number of columns is equal to the number of features of the object).\n        : Return one-dimensional numpy.ndarray-array that describes the probabilities of assigning input objects to the first class\n        (The number of elements of this array is equal to the number of rows of the matrix X, that is, the number of input objects).\n        '''\n        # Check that the logistic regression parameters (coefficients and free term) are not \"empty\"\n        if (self.__a is None) or (self.__b is None):\n            raise LogRegError('Parameters have not been specified!')\n        # Check that the input matrix X\n        if (X is None) or ((not isinstance(X, numpy.ndarray)) and (not isinstance(X, scipy.sparse.spmatrix))) or\\\n                (X.ndim != 2) or (X.shape[1] != self.__b.shape[0]):\n            raise LogRegError('Input data are wrong!')\n        # Calculate the desired probability array\n        # http://alturl.com/8kues\n        result = 1.0 / (1.0 + numpy.exp(-X.dot(self.__b) - self.__a))\n        print(\"transform\", result)\n        return result\n\n    def predict(self, X):\n        '''\n        Recognize which of the two classes are the input objects.\n        : Param X - a two-dimensional numpy.ndarray-array that describes the vectors of attributes of input objects\n        (One line is one characteristic vector, the number of rows is equal to the number of input objects,\n        The number of columns is equal to the number of features of the object).\n        : Return one-dimensional numpy.ndarray-array that describes the results of recognition of each of the input objects in the form\n        1 (the object belongs to the first class) or 0 (the object belongs to the second class). The number of elements in this array\n        Is equal to the number of rows of X; The number of input objects.\n        '''\n        return (self.transform(X) >= self.__th).astype(numpy.float)\n\n    def fit(self, X, y, eps=0.001, lr_max=1.0, max_iters = 1000):\n        '''\n        Teach logistic regression on a given training set by a gradient method.\n        : Param X is a two-dimensional numpy.ndarray-array that describes the vectors of the attributes of the input objects of the learning set\n        (One line - one characteristic vector, the number of rows is equal to the number of input objects, the number of columns\n        Is equal to the number of features of the object).\n        : Param y - one-dimensional numpy.ndarray-array that describes the desired results of recognition of each of the input\n        Objects of the training set in the form 1 (the object belongs to the first class) or 0 (the object belongs to the second class)\n        Class). The number of elements of this array is equal to the number of rows of the matrix X, i.e. The number of input objects.\n        : Param eps - the sensitivity of the algorithm to the change in the objective function (in our case, the logarithm of the function\n        Likelihood) after the next step of the algorithm. If the new value of the objective function does not exceed the old value\n        More than on eps or even less than the old value, then the training stops.\n        : Param lr_max is the maximum length of the learning rate coefficient (this coefficient will be adaptive, i.e.\n        Automatically selected at each step in the direction of the gradient, and the allowable range of changes is\n        [0; Lr_max]).\n        : Param max_iters - the maximum number of steps (iterations) of the learning algorithm. If the learning algorithm is executed\n        Max_iters steps, but the changes in the objective function are still great, i.e. The stop criterion is not met, then\n        The training stops anyway.\n        '''\n        # Check whether the training set is set correctly (if not, generate an exception)\n        if (X is None) or (y is None) or ((not isinstance(X, numpy.ndarray)) and\n                                              (not isinstance(X, scipy.sparse.spmatrix))) or\\\n                (X.ndim != 2) or (not isinstance(y, numpy.ndarray)) or (y.ndim != 1) or (X.shape[0] != y.shape[0]):\n            raise LogRegError('Train data are wrong!')\n        # Check whether the parameters of the learning algorithm are set correctly (if not, we generate an exception)\n        if (eps <= 0.0) or (lr_max <= 0.0) or (max_iters < 1):\n            raise LogRegError('Train parameters are wrong!')\n        # Initialize the free member and regression coefficients with random values\n        # Random values ​​are taken from the uniform distribution [-0.5, 0.5]\n        self.__a = numpy.random.rand(1)[0] - 0.5\n        self.__b = numpy.random.rand(X.shape[1]) - 0.5\n        # Calculate the log of the likelihood function at the starting point, i.e. Immediately after initialization\n        f_old = self.__calculate_log_likelihood(X, y, self.__a, self.__b)\n        print('{0:>5}\\t{1:>17.12f}'.format(0, f_old))\n        stop = False  # A flag indicating whether the stop criterion is fulfilled (at first it is not executed, of course)\n        iterations_number = 1  # count of the number of steps (iterations) of the algorithm\n        while not stop:  # until the break criterion is fulfilled, continue training\n            gradient = self.__calculate_gradient(X, y)  # calculate the gradient at the current point\n            print('fit gradient', gradient)\n            lr = self.__find_best_lr(X, y, gradient, lr_max)  # calculate the optimal step in the gradient direction\n            self.__a = self.__a + lr * gradient[0]  # correct the free member of the logistic regression\n            self.__b = self.__b + lr * gradient[1]  # correct logistic regression coefficients\n            # Logarithm of the likelihood function at a new point (with a new free term and new regression coefficients)\n            f_new = self.__calculate_log_likelihood(X, y, self.__a, self.__b)\n            print('{0:>5}\\t{1:>17.12f}'.format(iterations_number, f_new))\n            # If the log of the likelihood function has increased slightly or even decreased, then all is enough to learn\n            if (f_new - f_old) < eps:\n                stop = True\n            # If the log-likelihood of the likelihood function has increased substantially, then we check the number of steps of the algorithm\n            else:\n                f_old = f_new\n                iterations_number += 1  # increase the number of steps count\n                if iterations_number >= max_iters:  # if the number of steps in the algorithm is too large, then all\n                    stop = True\n        # Display the reason why the learning algorithm was completed\n        if iterations_number < max_iters:\n            print('The algorithm is stopped owing to very small changes of log-likelihood function.')\n        else:\n            print('The algorithm is stopped after the maximum number of iterations.')\n        self.__th = self.__calc_best_th(y, self.transform(X))\n\n    def __calculate_log_likelihood(self, X, y, a, b):\n        '''\n        Calculate the logarithm of the likelihood function on a given training set for given regression parameters\n        (Ie here as regression parameters - free term and coefficients - the corresponding\n        Method arguments, not the attributes of the class self .__ a and self .__ b).\n        : Param X is a two-dimensional numpy.ndarray-array that describes the vectors of the attributes of the input objects of the learning set\n        (One line - one characteristic vector, the number of rows is equal to the number of input objects, the number of columns\n        Is equal to the number of features of the object).\n        : Param y - one-dimensional numpy.ndarray-array that describes the desired results of recognition of each of the input\n        Objects of the training set in the form 1 (the object belongs to the first class) or 0 (the object belongs to the second class)\n        Class). The number of elements of this array is equal to the number of rows of the matrix X, i.e. The number of input objects.\n        : Param a is the free member of the logistic regression.\n        : Param b - one-dimensional numpy.ndarray-array of logistic regression coefficients.\n        : Return The logarithm of the likelihood function.\n        '''\n        eps = 0.000001 # small number that prevents zero under the logarithm\n        p = 1.0 / (1.0 + numpy.exp(-X.dot(b) - a))\n        print('__calculate_log_likelihood p', p)\n        return numpy.sum(y * numpy.log(p + eps) + (1.0 - y) * numpy.log(1.0 - p + eps))\n\n    def __calculate_gradient(self, X, y):\n        '''\n        Calculate the gradient from the log of the likelihood function on the given training set.\n        : Param X is a two-dimensional numpy.ndarray-array that describes the vectors of the attributes of the input objects of the learning set\n        (One line - one characteristic vector, the number of rows is equal to the number of input objects, the number of columns\n        Is equal to the number of features of the object).\n        : Param y - one-dimensional numpy.ndarray-array that describes the desired results of recognition of each of the input\n        Objects of the training set in the form 1 (the object belongs to the first class) or 0 (the object belongs to the second class)\n        Class). The number of elements of this array is equal to the number of rows of the matrix X, i.e. The number of input objects.\n        : Return The gradient from the log of the likelihood function, represented as a two-element tuple, is the first\n        The element of which is the partial derivative with respect to the free regression term (real number), and the second\n        Element - the vector of partial derivatives with respect to the corresponding regression coefficients (one-dimensional\n        Numpy.ndarray-array of real numbers).\n        '''\n        p = 1.0 / (1.0 + numpy.exp(-X.dot(self.__b) - self.__a))\n        da = numpy.sum(y - p)\n        db = X.transpose().dot(y - p)\n        return (da, db)\n\n    def __find_best_lr(self, X, y, gradient, lr_max):\n        '''\n        By the method of the golden section, find the optimal step of changing the regression parameters in the direction of the gradient\n        (I.e., the optimum learning rate coefficient).\n        : Param X is a two-dimensional numpy.ndarray-array that describes the vectors of the attributes of the input objects of the learning set\n        (One line - one characteristic vector, the number of rows is equal to the number of input objects, the number of columns\n        Is equal to the number of features of the object).\n        : Param y - one-dimensional numpy.ndarray-array that describes the desired results of recognition of each of the input\n        Objects of the training set in the form 1 (the object belongs to the first class) or 0 (the object belongs to the second class)\n        Class). The number of elements of this array is equal to the number of rows of the matrix X, i.e. The number of input objects.\n        : Param gradient - the gradient from the logarithm of the likelihood function, represented as a two-element tuple,\n        The first element of which is the partial derivative with respect to the free regression term (real number), and\n        The second element is the vector of partial derivatives with respect to the corresponding regression coefficients (one-dimensional\n        Numpy.ndarray-array of real numbers).\n        : Param lr_max is the maximum permissible rate of learning, i.e. Upper limit of search range\n        The optimal value of this coefficient (the lower bound is always zero).\n        : Return The optimal value of the learning speed coefficient (real number).\n        '''\n        lr_min = 0.0\n        theta = (1.0 + math.sqrt(5.0)) / 2.0\n        eps = 0.00001 * (lr_max - lr_min)\n        lr1 = lr_max - (lr_max - lr_min) / theta\n        lr2 = lr_min + (lr_max - lr_min) / theta\n        while abs(lr_min - lr_max) >= eps:\n            y1 = self.__calculate_log_likelihood(X, y, self.__a + lr1 * gradient[0], self.__b + lr1 * gradient[1])\n            y2 = self.__calculate_log_likelihood(X, y, self.__a + lr2 * gradient[0], self.__b + lr2 * gradient[1])\n            if y1 <= y2:\n                lr_min = lr1\n                lr1 = lr2\n                lr2 = lr_min + (lr_max - lr_min) / theta\n            else:\n                lr_max = lr2\n                lr2 = lr1\n                lr1 = lr_max - (lr_max - lr_min) / theta\n        return (lr_max - lr_min) / 2.0\n\n    def __calc_quality(self, y_target, y_real):\n        n = y_target.shape[0]\n        quality = {'tp': 0, 'tn': 0, 'fp': 0, 'fn': 0}\n        for ind in range(n):\n            if y_target[ind] > 0.0:\n                if y_real[ind] > 0.0:\n                    quality['tp'] += 1\n                else:\n                    quality['fn'] += 1\n            else:\n                if y_real[ind] > 0.0:\n                    quality['fp'] += 1\n                else:\n                    quality['tn'] += 1\n        return quality\n\n    def __calc_best_th(self, y_target, y_real):\n        best_th = 0.0\n        min_dist = 1.0\n        for th in map(lambda a: float(a) / 100.0, range(101)):\n            quality = self.__calc_quality(y_target, (y_real >= th).astype(numpy.float))\n            tpr = float(quality['tp']) / float(quality['tp'] + quality['fn'])\n            fpr = float(quality['fp']) / float(quality['tn'] + quality['fp'])\n            dist = math.sqrt((0.0 - fpr) * (0.0 - fpr) + (1.0 - tpr) * (1.0 - tpr))\n            if dist < min_dist:\n                min_dist = dist\n                best_th = th\n        return best_th\n\ndef load_mnist_for_demo(sparse=False):\n    '''\n    Load MNIST data to demonstrate the use of logistic regression for recognition\n    Handwritten figures from 0 to 9 (total ten classes, 60 thousand teaching pictures and 10 thousand test pictures).\n    : Param sparse - a flag indicating whether to represent a set of feature vectors in the form of a sparse matrix\n    Scipy.sparse.csr_matrix or as an ordinary matrix numpy.ndarray.\n    : Return A tuple of two elements: a learning set and a test set. Each of the sets - both teaching and\n    Test - is also specified as a two-element tuple, the first element of which is a set of vectors\n    Attributes of input objects (two-dimensional numpy.ndarray-array, the number of rows in which is equal to the number of input objects, and\n    The number of columns is equal to the number of features of the object), and the second element is the set of desired output signals for\n    Each of the corresponding input objects (one-dimensional numpt.ndarray-array, the number of elements in which is equal to the number\n    Input objects).\n    '''\n    from sklearn.datasets import fetch_mldata  # import a special module from the library ScikitLearn\n    # Load MNIST from the current directory or the Internet, if in the current directory of this data there is no\n    mnist = fetch_mldata('MNIST original', data_home='.')\n    # We get and normalize the feature vectors for the first 60 thousand pictures from MNIST used for training\n    # (Pixel brightness matrix 28x28 -> one-dimensional feature vector 784)\n    if sparse:\n        X_train = scipy.sparse.csr_matrix(mnist.data[0:60000].astype(numpy.float) / 255.0)\n    else:\n        X_train = mnist.data[0:60000].astype(numpy.float) / 255.0\n    y_train = mnist.target[0:60000]  # get the desired outputs (numbers from 0 to 9) for 60,000 training pictures\n    # We get and normalize the feature vectors for the next 10 thousand pictures from MNIST used for testing\n    # (Pixel brightness matrix 28x28 -> one-dimensional feature vector 784)\n    if sparse:\n        X_test = scipy.sparse.csr_matrix(mnist.data[60000:].astype(numpy.float) / 255.0)\n    else:\n        X_test = mnist.data[60000:].astype(numpy.float) / 255.0\n    y_test = mnist.target[60000:]  # get the desired outputs (numbers from 0 to 9) for 10 thousand test images\n    return ((X_train, y_train), (X_test, y_test))\n\n\nif __name__ == '__main__':\n    # If we use this module as the main module, and not just as a Python library, then run the demo on MNIST\n    import os.path  # we import a standard module for working with files\n    train_set, test_set = load_mnist_for_demo(True)  # load learning and test data MNIST\n    # For 10-class classification create 10 binary (2-class) classifiers based on logistic regression\n    classifiers = list()\n    for recognized_class in range(10):\n        classifier_name = 'log_reg_for_MNIST_{0}.txt'.format(recognized_class)\n        new_classifier = LogisticRegression()\n        if os.path.exists(classifier_name):\n            new_classifier.load(classifier_name)\n        else:\n            new_classifier.fit(train_set[0], (train_set[1] == recognized_class).astype(numpy.float))\n            new_classifier.save(classifier_name)\n        classifiers.append(new_classifier)\n    # On the test set, we calculate the results of recognition of figures by a team of 10 trained logistic regressions\n    # (The principle of decision making by such a collective: the input vector of attributes is considered to be related to that class whose\n    # Logistic regression gave the highest probability).\n    n_test_samples = test_set[0].shape[0]\n    outputs = numpy.empty((n_test_samples, 10), dtype=numpy.float)\n    for recognized_class in range(10):\n        outputs[:, recognized_class] = classifiers[recognized_class].transform(test_set[0])\n    results = outputs.argmax(1)\n    # Compare the results obtained with the reference ones and estimate the percentage of errors of the collective of logistic regressions\n    n_errors = numpy.sum(results != test_set[1])\n    print('Errors on test set: {0:%}'.format(float(n_errors) / float(n_test_samples)))\n```\n\nmore examples:\nhttps://github.com/Samurais/maximum-likelihood-estimation\n\n# references\n\n[Likelihood_function](https://en.wikipedia.org/wiki/Likelihood_function)\n\n[似然函数](https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0)\n\n[Likelihood & LogLikelihood](https://onlinecourses.science.psu.edu/stat504/node/27)\n\n[Maximum likelihood linear regression tensorflow\n](https://stackoverflow.com/questions/41885665/maximum-likelihood-linear-regression-tensorflow)\n\n[What is the difference between “likelihood” and “probability”?](https://stats.stackexchange.com/questions/2641/what-is-the-difference-between-likelihood-and-probability)",metaData:{layout:"post",title:"最大似然估计",excerpt:"最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。",category:"research",tags:["algorithm"],disqus:!0}}}});