webpackJsonp([1,43],{328:function(e,n){e.exports={rawContent:"\n\n> From http://hmmlearn.readthedocs.io/en/latest/tutorial.html\n\nThe HMM is a generative probabilistic model, in which a sequence of observable **X** variables is generated by a sequence of internal hidden states **Z**. The hidden states are not be observed directly. The transitions between hidden states are assumed to have the form of a (first-order) Markov chain. They can be specified by the start probability vector **π** and a transition probability matrix **A**. The emission probability of an observable can be any distribution with parameters **θ** conditioned on the current hidden state. The HMM is completely determined by **π**, **A** and **θ**.\n\n![](http://7xkeqi.com1.z0.glb.clouddn.com/chatbot/images/2017/06/hmm-hidden-weather-example.gif)\n\n举例来说，假设对于隐居室内的人，天气是隐含状态，海藻是可观察状态，海藻的状态和天气之间存在联系，通过观察海藻的状态来预测天气的状态。\n\n如果使用HMM模型，我们需要知道天气的初始状态，天气的状态转移矩阵和海藻的状态转移矩阵。\n\n## 概念\n* A - 隐含状态转移概率矩阵。\n描述了HMM模型中各个状态之间的转移概率。其中Aij = P( Sj | Si ),1≤i,,j≤N，表示在 t 时刻、状态为 Si 的条件下，在 t+1 时刻状态是 Sj 的概率。\n\n* θ - 观测状态转移概率矩阵，也称为混淆矩阵\n令N代表隐含状态数目，M代表可观测状态数目，则：θij = P( Oi | Sj ), 1≤i≤M,1≤j≤N，表示在 t 时刻、隐含状态是 Sj 条件下，观察状态为 Oi 的概率。\n\n* π - 初始状态概率矩阵\n表示隐含状态在初始时刻t=1的概率矩阵，(例如t=1时，P(S1)=p1、P(S2)=P2、P(S3)=p3，则初始状态概率矩阵 π=[ p1 p2 p3 ].\n\n对于包含M个客观察状态和N个隐含状态的HMM模型来说，用**λ={ π, A, θ }**来表示HMM的参数。\n\n## HMM可以解决的问题\n\n* 根据可观察状态的序列找到一个最可能的隐藏状态序列\n\n一个广泛使用的例子，就是使用HMM+Viterbi算法完成词性标注任务。\n\n* 已知模型参数，计算某一给定可观察状态序列的概率\n\n*  根据观察到的序列集来找到一个最有可能的 HMM\nGiven just the observed data, estimate the model parameters.\n\nThe first and the second problem can be solved by the dynamic programming algorithms known as the Viterbi algorithm and the Forward-Backward algorithm, respectively. The last one can be solved by an iterative Expectation-Maximization (EM) algorithm, known as the Baum-Welch algorithm.\n\n\n## 三个重要假设\n这三个假设并不现实。\n\n假设1：马尔可夫假设（状态构成一阶马尔可夫链）\n![](http://7xkeqi.com1.z0.glb.clouddn.com/chatbot/images/2017/06/hmm-hidden-1.png)\n\n假设2：不动性假设（状态与具体时间无关）\n![](http://7xkeqi.com1.z0.glb.clouddn.com/chatbot/images/2017/06/hmm-hidden-2.png)\n\n假设3：输出独立性假设（输出仅与当前状态有关）\n![](http://7xkeqi.com1.z0.glb.clouddn.com/chatbot/images/2017/06/hmm-hidden-3.png)\n\n# Refers\n\n[Rabiner89]\t Lawrence R. Rabiner “A tutorial on hidden Markov models and selected applications in speech recognition”, Proceedings of the IEEE 77.2, pp. 257-286, 1989.\n\n[Bilmes98]\tJeff A. Bilmes, “A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hidden Markov models.”, 1998.\n\n[隐马尔可夫模型（HMM）攻略](http://blog.csdn.net/likelet/article/details/7056068)\n\n\n\n\n",metaData:{layout:"post",title:"隐马尔科夫模型",excerpt:"隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。",category:"research",tags:["probability","algorithm"],disqus:!0}}}});